%!TEX root = ..\Main.tex
\chapter{Introduction}
In later years research within HCI has shifted towards User Experience(UX) and the actual experience with a system.
User Experience is usually measured and analyzed through subjective measures, such as Self-Assessment Manikin (SAM), expert analysis, Think-Aloud and Cued-Recall Debrief.
Although well-established and renowned, such methods can take a substantial amount of time to perform, and are error prone with regards to subjective analysis, such as Peak-End rule and expert bias.
In recent years, interesting research has emerged, surrounding objective measurement gathered through sensors attached to the human body, and subsequent analysis of that data to estimate a users state of mind or user experience in some form or another.
This is interesting because of the potential rewards using objective data rather than subjective data.
If it is possible to accurately predict a person's affective state, and apply such knowledge within usability testing, then issues like peak-end and expert bias could be removed.
Further by analyzing data, rather than video material of test participant under interview, could drastically reduce the amount of time required to perform a user experience evaluation.
Most research within this area has focused solely on single sensors as data providers, and as such it would be
interesting to see if any improvements could be made using multiple sensors, coupled with techniques from machine intelligence to fuse the results.
Fusion multiple sensors could possibly yield better results, or increase the reliability of cheap, consumer grade hardware.

This work explores both aspects of these considerations - is it possible to predict affective states, and with increased
significance using multiple sensors, and could such knowledge be applied in a context such as usability testing?
