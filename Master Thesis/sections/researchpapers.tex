%!TEX root = ..\Main.tex
\chapter{Research Papers}
This chapter presents two research papers, both produced during our Master
Thesis semester. The first paper is produced in collaboration with another
workgroup, and the second produced entirely by us.

The first paper is a continuation of our work during 9th semester which includes
a significant literature review investigating the occurrence of research making
use of physiological measurement, and in particular research relying on fusing
several physiological measurements, in order to identify emotions and affective
state. Our 9th semester project formulates a setup and in it we conduct a
proof-of-concept experiment and arrive at promising results. The first paper
scales the experimental setup substantially, in order to more convincingly
confirm or reject its hypothesis.

The second paper builds upon the first, and aims to use its results in a more
specific manner: to identify usability problems solely by physiological
measurements. Based on the assumption from related work, that usability problems
induce a negative affective state, often discretized as \textit{frustration},
our aim was to identify such states and thereby usability problems.

Below is a short summary of each paper along with its research questions.
Followed by that, is a methodical reflection, wherein we scrutinize and discuss
our findings and methods. Each paper can be found in its full length in the
Appendix.

\section{Research paper 1}
\paragraph{Title:}
Real-time Measurement of User Experience
\paragraph{Hypothesis:}
\begin{itemize}
    \item \textbf{H1:} There is a statistically significant
      correlation between subjective SAM ratings and physiological
      measurements from consumer-grade sensors.
    \item \textbf{H2:} Statistically, fusion of consumer-grade sensors
      has a significantly higher prediction rate than each sensor
      individually.
\end{itemize}
\paragraph{Summary:}
%\begin{itemize}
%\item What is the field of study
%\item What is the paper trying to investigate
%\item How did it investigate
%\item Explain experiment
%\item Present results
%\end{itemize}

The paper is based on our 9th semester project, which consisted primarily of an
elaborate literature review within the field of HCI. The purpose was to
investigate the use of sensors within UX research and experiments. In
particular, to investigate their use in eliciting emotions or affective state in
UX test participants. The argument for using sensors in such cases, is that
eliciting how test participants feel during an experiment is often left to be
evaluated by third-party experts, leaving room for subjective assessments. Using
sensors could potentially mitigate subjectivity, and help elicit more objective
truths about how test participants feel and react during experiments.

The conclusion of the literature review was that while the use of sensors has increased in recent years,
it is still not the predominant method, or even a particular popular method for
eliciting emotions. Furthermore, the use of multiple sensor was even
rarer. Although not always mentioned explicitly in the reviewed material, it was
sometimes stated that further work was needed, additionally results between studies varied
in accuracy. The reviewed material also proposed disparate methods, rarely
agreeing on how sensor data should be processed, or fused in the case of
multiple sensors.

Our 9th semester project was motivated by the presumption that more promising
results could be produced by using multiple sensors and fusing their results. It
also proposed the use of consumer-grade hardware in order to make our findings
more accessible for others to replicate. Common Machine Learning techniques and statistical methods for
data-processing were proposed as well, again to make our findings easily
reproducible and accessible. The project describes a setup for experiments onto
which the above can be applied. The setup involves multiple sensors such as GSR,
EEG and heart-rate monitoring, and emotions were artificially induced using
images having positive, neutral and negative connotations, selected from the
reputable IAPS collection. However, while promising results were found, a lack
in test participants make them particularly case-specific, and further study was
required in order to verify them.

This paper is, as mentioned, based on the above preliminary research. The
primary motivation for the work was to remedy shortcomings in our 9th semester
project. In particular, with an increased number of test participants, our aim
was to confirm that using multiple sensors and fusing their data together using machine learning produce better results, than using a single sensors data. Refining the
setup and conducting it in a proper testing environment yielded a more
controlled experiment. In particular, our test-software was significantly
revised and more thoroughly tested for bugs, additional literature was consulted
with regards to feature extraction for the ML technique and the experiment procedure was
adjusted and formalized. Due to the experiment now being conducted in a
usability lab, we took advantage of the additional video-recording capabilities,
and recorded both participants' facial expressions, the computer screen while
they interacted with the software, and the setup from behind-the-shoulders of
the test participants. And with a substantial increase in participants, we
produced similar but more convincing results compared to our previous work.

In conclusion, it was confirmed that using multiple sensors, fusing their input
using ML techniques, produces more accurate results compared to each individual
sensor. Furthermore, we found the assumption that self-reported SAM ratings
during the experiment correlated with the physiological measurements taken
simultaneously, to be true.

\subsection{Methodical Reflection:}
In this section, we discuss the methods and practices we applied in this paper,
which by extension also includes the preliminary work in our 9th semester
project.

An area within our study which require scrutiny and further investigation is
that of context. Context is a broad term and involves many controllable and
uncontrollable variables, for instance; the mood test participants are in
before and under the experiment, the lighting and temperature in the room, the
particular setup of the experiment, etc. We acknowledge that the above can
affects how participants interact with the setup, and particularly how they
react to the stimuli. If it had any significant impact on our results is
unknown, and investigating it would require making a study specifically targeting this unexplored area of the current ongoing research within HCI.
\todo{link this somehow to Scherer and his components}

Self-Assessment Manikin, the method we used during experiments to get subjective
ratings from participants on how they reacted to the stimuli, has been
challenged in other studies, and our use of it could therefore be challenged as
well. For one, the method is highly subjective: test participants can lie,
misunderstand the method, or simply have widely different frames of
reference. It is particularly the two latter cases that has been
scrutinized. Being \textit{moderately calm} might be answered differently from
between participants, and each participant might have difficulties mapping
correctly to the scales if stimuli with increasingly extreme connotations are
presented. However, taking the fallacies above into consideration, the SAM
method is still highly regarded and widely used, as evident from our review of
literature within the field of HCI.
\todo{confirm the above with sources. I have read this somewhere, I just need to
  find it again, (source is fusion4)}

The machine learning techniques we have used are generally basic ``off-the-shelf'' solutions to the problem area we are dealing with.
Due to the scope of the study we are conducting, it is not feasible to dive into specific techniques which could be used.
The results achieved could most likely be substantially improved by using better techniques, but finding the optimal setup for ML for each sensor would most likely be a study in its own right.

%Things to discuss:
%\todo{below is just a list of things we could discuss in this section}
%\begin{itemize}
%\item context
%\item Scherer
%\item misunderstanding SAM
%\item results are held against test participants own subjective SAM ratings
%\item the one-one, one-many etc. statement that I can't remember where I read
%  (perhaps liapis)
%\item
%\end{itemize}

\section{Research paper 2}
\paragraph{Title:}
Real-time Detection of User Experience Problems
\paragraph{Hypothesis:}
\begin{itemize}
    \item \textbf{H1:} somethign
    \item \textbf{H2:} somethign
\end{itemize}
\paragraph{Summary:}
The paper is based on the findings from our first paper. While the first paper focused on verifying the hypothesis that
it is possible to predict affective states from physiological data, the test setup was also focused around this
hypothesis.  Instantaneous stimuli was selected, and for each of the stimuli groups \textit{negative}, \textit{neutral}
and \textit{positive} only the most extreme pictures were used.  This was very case-specific, and unlikely to be
experienced in more general cases, or real-word scenarios.

The aim for the second paper, was to apply techniques and knowledge gained from the previous work in a more practical
setting. The idea was to apply it within usability testing, in an attempt to mitigate some of the subjectivity bias
associated with performing such tests, by predicting usability problems based on physiological data. Related work was
reviewed to find common affective states associated with experiencing usability problems and computer tasks most likely
to induces such state. Frustration was found mentioned often, and working with email-related tasks to be inducing
frustration the most. Based on this, a ``mock'' email client was developed, and a framework to deliberately seed
usability problems into it. The client was kept as simple as possible, in order to avoid unintentionally introducing
usability problems.

A usability test of the ``mock'' email client was performed. However unlike traditional usability testing, no usability
expert was present during testing, and feedback was entirely dependent on physiological data captured via sensors during
testing.  A total of 11 tasks were formulated, 7 of which had seeded usability problems.  The same physiological data
was collected as in the first paper, meaning GSR, EEG, heart rate and facial features.  The techniques used for finding
usability problems changed from a prediction oriented approach to a novelty detection approach.

In order to predict usability problems, a one-class SVM was trained on \textit{neutral} data, presumably containing no
usability problems. Subsequent \textit{unknown} data was predicted upon to identify possible usability problems.

\todo{Når vi har mere resultat og metode, så skriv det ind her}

\subsection{Methodical Reflection:}
The experiment setup was controlled to the best of our abilities and conducted inside a usability lab. A changing
variable we deliberately constructed in the system, was to randomize the order in which tasks are presented to the user,
i.e. the order differs between test participants.
This obviously leave us with the same concerns as in paper 1, that context has not thoroughly been investigated and taken
into consideration. As with the first paper, this was simply dismissed as a limitation and scope of this paper.

Specific to this test, the program developed is also a factor to consider when looking at the results.  We attempted to
make a program with no usability errors, but as research has shown since the dawn of HCI, this is nearly impossible.  We
experienced problems with the computer used for the test, where the program would have unexpected latency and unresponsiveness when using the keyboard and mouse. This was not intentional, and hence not a seeded usability problem.
The user might however, still perceive the unresponsiveness of the program as a usability problem. 
This was the case in at least one test.  
Further we found small areas of the program that could be considered cosmetic usability problems, however, as the paper does not actively try
to estimate severity this was left out in the evaluation. Other concerns became evident when considering how each
individual seeded problem was perceived by the test participant. Some problems were perceived immediately,
e.g. participants were paying attention when error feedback was given, while others problems were not. In particular,
tasks that required test participants to use the keyboard, usually draws their attention to it, and away from any
feedback signifying that an error has occurred. An implication that follows from this, is that it becomes harder to
determine exactly \textit{when} a test participant experiences a particular seeded problem.

One of the biggest methodical complications of this paper is the fact that no formalized methods exists for evaluation
user experience problems, in contrast to usability problems. We defined our own user experience problem as being a
usability problem detectable from affective state, but with no information regarding severity with regards to the common
metrics like task completion time and effort required. Further there is, to the best of our knowledge, no conclusive
literature concerning finding or defining how to find user experience problems using physiological data. We made an
attempt at defining it from previous literature concerning frustration and the abstract idea of a usability problem
leading to frustration, but as said, this is not a conclusive formalized method. Creating this would require significant
research with that area of interest being the specific purpose of the research. Further looking at the general opinions
between researchers, a definitive conclusion seems to be some ways out as simply agreeing on what an affective state
truly consists of is very opinionated.\todo{Hele det her skal nok skrives om / smides ud.}

The method used in this paper is \textit{novelty detection} using a one-class SVM. The primary requirements for
achieving good performance, i.e. good predictive power, is to ensure that training data contains as few anomalies as
possible. The difficulty associated with ensuring this can vary depending on the kind of data considering, but in our
case, where we consider physiological data, is is not trivial. We attempt to minimize the risk of
training on data containing anomalies by considering only data from the first two tasks, containing no seeded
problems. However, we cannot be certain that this data never contains any anomalies, as identifying anomalies is what we
are attempting in the first place. This is to say, that our ability to identify anomalies is at most as good as the data
we train on. In order to ensure a better set of training data, it would be better to create a larger set of tasks, and verify those tasks and the software they are conducted in as being usability error free. 
The robustness of such a set of tasks would be better if such initial evaluation had been done.

A recurring concern, also present during the first paper and our 9th semester project, is that of finding the
\textit{correct} parameters and features for the classification algorithm. In an attempt to mitigate these concerns, we
performed grid searching, i.e. a near-exhaustive search, on parameters. However, doing so is considerably time consuming
and dependent on chosen features. This means, that although we can search for optimal parameters for a set of features,
a new search has to be performed if a new set of features are considered. This is to say, that validating if a set of
features yields good results, e.g. many true-positives and few false-positives, is considerably time consuming. Features
suggested in related work are disparate, i.e. varying time-spans and extracted from various statistics, and we have not
been able to find conclusive indications as to which features should be used. We have attempted to uncover differences
in features by considering two different sets of features - one considering short-term reactions and another considering
long-term reactions, governed by the time-spans considered. We present the findings under a discussion in the paper. We
want to make it clear, that finding optimal features and classification parameters, is not trivial. 
It is also debateable how robost the model is given we search for the optimal solutions. 
We have the \textit{best} model for a specific set of features, for a specific set of data. 
We cannot take this model and use it on another person. 

%remember to mention:
    %that training data has to be clean
    %specifying windows to specify as usability problems occurring
    %weighting individual sensors (what is a hit?)
    %scoring
    %features and time
    %svm params, grid search relies on the features selected
    %disparate opinions on time to consider for gsr

