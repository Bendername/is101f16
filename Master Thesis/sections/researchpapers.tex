%!TEX root = ..\Main.tex
\chapter{Research Papers}
This chapter presents two research papers, both produced during our Master
Thesis semester. The first paper is produced in collaboration with another
workgroup, and the second produced entirely by us.

The first paper is a continuation of our work during 9th semester which includes
a significant literature review investigating the occurrence of research making
use of physiological measurement, and in particular research relying on fusing
several physiological measurements, in order to identify emotions and affective
state. Our 9th semester project formulates a setup and in it we conduct a
proof-of-concept experiment and arrive at promising results. The first paper
scales the experimental setup substantially, in order to more convincingly
confirm or reject its hypothesis.

The second paper builds upon the first, and aims to use its results in a more
specific manner: to identify usability problems solely by physiological
measurements. Based on the assumption from related work, that usability problems
induce a negative affective state, often discretized as \textit{frustration},
our aim was to identify such states and thereby usability problems.

Below is a short summary of each paper along with its research questions.
Followed by that, is a methodical reflection, wherein we scrutinize and discuss
our findings and methods. Each paper can be found in its full length in the
Appendix.

\section{Research paper 1}
\paragraph{Title:}
Real-time Measurement of User Experience
\paragraph{Hypothesis:}
\begin{itemize}
    \item \textbf{H1:} There is a statistically significant
      correlation between subjective SAM ratings and physiological
      measurements from consumer-grade sensors.
    \item \textbf{H2:} Statistically, fusion of consumer-grade sensors
      has a significantly higher prediction rate than each sensor
      individually.
\end{itemize}
\paragraph{Summary:}
%\begin{itemize}
%\item What is the field of study
%\item What is the paper trying to investigate
%\item How did it investigate
%\item Explain experiment
%\item Present results
%\end{itemize}

The paper is based on our 9th semester project, which consisted primarily of an
elaborate literature review within the field of HCI. The purpose was to
investigate the use of sensors within UX research and experiments. In
particular, to investigate their use in eliciting emotions or affective state in
UX test participants. The argument for using sensors in such cases, is that
eliciting how test participants feel during an experiment is often left to be
evaluated by third-party experts, leaving room for subjective assessments. Using
sensors could potentially mitigate subjectivity, and help elicit more objective
truths about how test participants feel and react during experiments.

The conclusion of the literature review was that while the use of sensors has increased in recent years,
it is still not the predominant method, or even a particular popular method for
eliciting emotions. Furthermore, the use of multiple sensor was even
rarer. Although not always mentioned explicitly in the reviewed material, it was
sometimes stated that further work was needed, additionally results between studies varied
in accuracy. The reviewed material also proposed disparate methods, rarely
agreeing on how sensor data should be processed, or fused in the case of
multiple sensors.

Our 9th semester project was motivated by the presumption that more promising
results could be produced by using multiple sensors and fusing their results. It
also proposed the use of consumer-grade hardware in order to make our findings
more accessible for others to replicate. Common Machine Learning techniques and statistical methods for
data-processing were proposed as well, again to make our findings easily
reproducible and accessible. The project describes a setup for experiments onto
which the above can be applied. The setup involves multiple sensors such as GSR,
EEG and heart-rate monitoring, and emotions were artificially induced using
images having positive, neutral and negative connotations, selected from the
reputable IAPS collection. However, while promising results were found, a lack
in test participants make them particularly case-specific, and further study was
required in order to verify them.

This paper is, as mentioned, based on the above preliminary research. The
primary motivation for the work was to remedy shortcomings in our 9th semester
project. In particular, with an increased number of test participants, our aim
was to confirm that using multiple sensors and fusing their data together using machine learning produce better results, than using a single sensors data. Refining the
setup and conducting it in a proper testing environment yielded a more
controlled experiment. In particular, our test-software was significantly
revised and more thoroughly tested for bugs, additional literature was consulted
with regards to feature extraction for the ML technique and the experiment procedure was
adjusted and formalized. Due to the experiment now being conducted in a
usability lab, we took advantage of the additional video-recording capabilities,
and recorded both participants' facial expressions, the computer screen while
they interacted with the software, and the setup from behind-the-shoulders of
the test participants. And with a substantial increase in participants, we
produced similar but more convincing results compared to our previous work.

In conclusion, it was confirmed that using multiple sensors, fusing their input
using ML techniques, produces more accurate results compared to each individual
sensor. Furthermore, we found the assumption that self-reported SAM ratings
during the experiment correlated with the physiological measurements taken
simultaneously, to be true.

\subsection{Methodical Reflection:}
In this section, we discuss the methods and practices we applied in this paper,
which by extension also includes the preliminary work in our 9th semester
project.

An area within our study which require scrutiny and further investigation is
that of context. Context is a broad term and involves many controllable and
uncontrollable variables, for instance; the mood test participants are in
before and under the experiment, the lighting and temperature in the room, the
particular setup of the experiment, etc. We acknowledge that the above can
affects how participants interact with the setup, and particularly how they
react to the stimuli. If it had any significant impact on our results is
unknown, and investigating it would require making a study specifically targeting this unexplored area of the current ongoing research within HCI.
\todo{link this somehow to Scherer and his components}

Self-Assessment Manikin, the method we used during experiments to get subjective
ratings from participants on how they reacted to the stimuli, has been
challenged in other studies, and our use of it could therefore be challenged as
well. For one, the method is highly subjective: test participants can lie ,
misunderstand the method, or simply have widely different frames of
reference. It is particularly the two latter cases that has been
scrutinized. Being \textit{moderately calm} might be answered differently from
between participants, and each participant might have difficulties mapping
correctly to the scales if stimuli with increasingly extreme connotations are
presented. However, taking the fallacies above into consideration, the SAM
method is still highly regarded and widely used, as evident from our review of
literature within the field of HCI.
\todo{confirm the above with sources. I have read this somewhere, I just need to
  find it again, (source is fusion4)}

The machine learning techniques we have used are generally basic ``off-the-shelf'' solutions to the problem area we are dealing with.
Due to the scope of the study we are conducting, it is not feasible to dive into specific techniques which could be used.
The results achieved could most likely be substantially improved by using better techniques, but finding the optimal setup for ML for each sensor would most likely be a study in its own right.

%Things to discuss:
%\todo{below is just a list of things we could discuss in this section}
%\begin{itemize}
%\item context
%\item Scherer
%\item misunderstanding SAM
%\item results are held against test participants own subjective SAM ratings
%\item the one-one, one-many etc. statement that I can't remember where I read
%  (perhaps liapis)
%\item
%\end{itemize}

\section{Research paper 2}
\paragraph{Title:}
Real-time Detection of User Experience Problems
\paragraph{Hypothesis:}
\begin{itemize}
    \item \textbf{H1:} somethign
    \item \textbf{H2:} somethign
\end{itemize}
\paragraph{Summary:}
The paper is largely based on the findings of the first paper. With the technique and methods verified a natural continuation of the research field was done.
While the first paper was focused around the verification of the actual hypothethis that it is possible to find affective states from physiological data, the test setup was also focused around this hypothesis.
Instantaneous stimuli was selected, and for each of the stimuli groups \textit{negative}, \textit{neutral} and \textit{positive} only the most extreme pictures were used.
This was obviously a short-comming in terms of a ``real world'' scenario, where the exposure would not be as tailored, nor extreme.
This was the main target for the second paper, selecting a real world scenario where the technique could be used.
Literature was reviewed to find common areas surrounding usability problems, and an email client was selected as the framework for which to try and measure affective state.
In order to control the experiment and verify our findings, we developed a simple program to avoid usability problems to the best of our knowledge.
Specific problems were then seeded into the program, and made active only during specific tasks.
A total of 11 tasks were completed, and of those 7 had seeded usability problems.
The same physiological data was collected as in the first paper, so GSR, EEG, heart rate and facial features.
The techniques used for finding usability problems changed from a prediction oriented approach to a novelty detection approach.\todo{Når vi har mere resultat og metode, så skriv det ind her}
\subsection{Methodical Reflection:}
The entire setup was controlled just as the first test. The usability lab was used, and the only changing variables in the system was the order of the seeded problems (which were random). This obviously leave us with the same problem as in paper 1, that context has not thoroughly been investigated and taken into consideration. As with the first paper, this was simply dismissed as a limitation and scope of this paper.

Specific to this test the program developed is also a factor to consider when looking at the results.
We attempted to make a program with no usability errors, but as research has shown since the dawn of HCI this is nearly impossible.
We experienced problems with the computer used for the test, where the program would lag. While this is not specifically a programmatic usability problem, the user will still perceive the unresponsiveness of the program as a usability problem. This was the case in at least one test.
Further we found small areas of the program that could be considered cosmetic usability problems, however, as the paper does not actively try to estimate severity this was left out in the evaluation.

One of the biggest methodical complications of this paper is the fact that no formalized methods exists for evaluation user experience problems. We defined our own user experience problem as being a usability problem detectable from affective state, but with no information regarding severity with regards to the common metrics like task completion time and effort required. Further there was, to the best of our knowledge, no conclusive literature concerning finding or defining how to find user experience problems using physiological data. We made an attempt at defining it from previous literature concerning frustration and the abstract idea of a usability problem leading to frustration, but as said, this is not a conclusive formalized method. Creating this would require significant research with that area of interest being the specific purpose of the research. Further looking at the general opinions between researchers, a definitive conclusion seems to be some ways out as simply agreeing on what an affective state truly consists of is very opinionated.
