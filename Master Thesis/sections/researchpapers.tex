%!TEX root = ..\Main.tex
\chapter{Research Papers}
This chapter presents two research papers, both produced during our Master
Thesis semester. The first paper and experiment for the first is produced in collaboration with another
master thesis group, namely is102f16, whereas the second paper is produced entirely by us and the experiment is produced in collaboration with is102f16.

The first paper is a combined effort where a large study with a large scale test was conducted.
The aim was to classify the affective state from test participants, using images as stimuli.
Physiological data was collected through a self developed program which allowed synchronization of all the data.
The test participants also reported arousal and valence values for the stimuli, which was used to is test the validity of the classifier.

The second paper builds upon the first, and aims to use its results in a more
specific manner: to detecting usability problems from physiological
data. Based on the assumptions from related work, that usability problems
induce a negative affective state, often discretized as \textit{frustration},
our aim was to identify such states and thereby usability problems.

Below is a short summary of each paper along with its research questions.
Followed by that, is a methodical reflection, wherein we scrutinize and discuss
our findings and methods. Each paper can be found in its full length in the
Appendix.

\section{Research paper 1}
\paragraph{Title:}
Real-time Measurement of User Experience
\paragraph{Hypothesis:}
\begin{itemize}
    \item \textbf{H1:} There is a statistically significant
      correlation between subjective SAM ratings and physiological
      measurements from consumer-grade sensors.
    \item \textbf{H2:} Statistically, fusion of consumer-grade sensors
      has a significantly higher prediction rate than each sensor
      individually.
\end{itemize}
\paragraph{Summary:}
%\begin{itemize}
%\item What is the field of study
%\item What is the paper trying to investigate
%\item How did it investigate
%\item Explain experiment
%\item Present results
%\end{itemize}

The paper is based on our 9th semester project, which consists primarily of an
elaborate literature review within the field of HCI. The purpose was to
investigate the use of sensors within UX research and experiments. In
particular, to investigate their use in eliciting emotions or affective state in
UX test participants. The argument for using sensors in such cases, is that
eliciting how test participants feel during an experiment is often left to be
evaluated by third-party experts, leaving room for subjective assessments. Using
sensors could potentially mitigate subjectivity, and help elicit more objective
truths about how test participants feel and react during experiments.

The primary motivation for the first paper was to create a much more robust study than our 9th semester.
This included a complete code rewrite of the interface which collected physiological data, such that it was more robust while running for a long duration. We had to handle things such as hardware not responding and comming back online, while still keeping the collected data synchronized.
There was also a revision to the test setup. It was moved to a controlled setting of a usability lab, where it was possible to collect more data beside the physiological.
The setup meant we collected video of the test participants legs, which enabled us to watch for movement related artifacts. We also collected video of the test participants face and the video of the monitor. 
Further audio was collected. 
The entire test was formalized such that it is easy replicable.
New features was selected for the classifier which was rooted in litterature.

In conclusion, it was confirmed that using multiple sensors, fusing their input
using ML techniques, produces more accurate results compared to each individual
sensor. Furthermore, we found the assumption that self-reported SAM ratings
during the experiment correlated with the physiological measurements taken
simultaneously, to be true.

\subsection{Methodical Reflection:}
An area within our study which require scrutiny and further investigation is
that of context. Context is a broad term and involves many controllable and
uncontrollable variables, for instance; the mood test participants are in
before and under the experiment, the lighting and temperature in the room, the
particular setup of the experiment, etc. We acknowledge that the above can
affects how participants interact with the setup, and particularly how they
react to the stimuli. If it had any significant impact on our results is
unknown, and investigating it would require making a study specifically targeting this unexplored area of the current ongoing research within HCI.

Self-Assessment Manikin, the method we used during experiments to get subjective
ratings from participants on how they reacted to the stimuli, has been
challenged in other studies, and our use of it could therefore be challenged as
well. For one, the method is highly subjective: test participants can lie,
misunderstand the method, or simply have widely different frames of
reference. It is particularly the two latter cases that has been
scrutinized. Being \textit{moderately calm} might be answered differently from
between participants, and each participant might have difficulties mapping
correctly to the scales if stimuli with increasingly extreme connotations are
presented. However, taking the fallacies above into consideration, the SAM
method is still highly regarded and widely used, as evident from our review of
literature within the field of HCI.\cite{fusion4}

The machine learning techniques we have used are generally ``off-the-shelf'' solutions to the problem area we are dealing with.
Due to the scope of the study we are conducting, it is not feasible to dive into specific techniques which could be used.
The results achieved could most likely be substantially improved by using better techniques, but finding the optimal setup for ML for each sensor is beyond the scope of this paper.

The results we arrive at find significantly better results on fusion with the technique stacking.
The results are promising in terms of the field being rather young, and only in recent years computational power has increased enough to make machine learning techniques more accessable to the average researcher. It is possible to run even advanced and complex algorithms in fairly short time on normal computers, which before required super computers. 
With advancements in both the techniques used and the hardware available, it could possibly be feesable to consistently measure affective changes in real time. 

%Things to discuss:
%\todo{below is just a list of things we could discuss in this section}
%\begin{itemize}
%\item context
%\item Scherer
%\item misunderstanding SAM
%\item results are held against test participants own subjective SAM ratings
%\item the one-one, one-many etc. statement that I can't remember where I read
%  (perhaps liapis)
%\item
%\end{itemize}

\section{Research paper 2}
\paragraph{Title:}
Real-time Detection of User Experience Problems
\paragraph{Hypothesis:}
\begin{itemize}
    \item \textbf{H1:} There is a statistically significant improvement using sensor fusion to detect usability problems compared to using individual sensors.
\end{itemize}
\paragraph{Summary:}
The paper is based on the findings from our first paper. While the first paper focused on verifying the hypothesis that
it is possible to predict affective states from physiological data, the test setup was also focused around this
hypothesis.  Instantaneous stimuli was selected, and for each of the stimuli groups \textit{negative}, \textit{neutral}
and \textit{positive} only the most extreme pictures were used.  This was very case-specific, and unlikely to be
experienced in more general cases, or real-word scenarios.

The aim for the second paper, was to apply techniques and knowledge gained from the previous work in a more practical
setting. The idea was to apply it within usability testing, in an attempt to mitigate some of the subjectivity bias
associated with performing such tests, by predicting usability problems based on physiological data. Related work was
reviewed to find common affective states associated with experiencing usability problems and computer tasks most likely
to induces such state. Frustration was found mentioned often, and working with email-related tasks to be inducing
frustration the most. Based on this, a ``mock'' email client was developed, and a framework to deliberately seed
usability problems into it. The client was kept as simple as possible, in order to avoid unintentionally introducing
usability problems.

A usability test of the ``mock'' email client was performed. However unlike traditional usability testing, no usability
expert was present during testing, and feedback was entirely dependent on physiological data captured via sensors during
testing.  A total of 11 tasks were formulated, 7 of which had seeded usability problems.  The same physiological data
was collected as in the first paper, meaning GSR, EEG, heart rate and facial features.  The techniques used for finding
usability problems changed from a prediction oriented approach to a novelty detection approach.

In order to predict usability problems, a one-class SVM was trained on \textit{neutral} data, presumably containing no
usability problems. Subsequent \textit{unknown} data was predicted upon to identify possible usability problems.

\todo{Når vi har mere resultat og metode, så skriv det ind her}

\subsection{Methodical Reflection:}
% mention that we changed from having a hypothesis
%    why did we do this -> more exploratory, cannot 'cook' down to just one number, too many uncertainties
\textit{Test and experiment}\\
The experiment setup was controlled to the best of our abilities and conducted inside a usability lab. A changing
variable we deliberately constructed in the system, was to randomize the order in which tasks are presented to the user,
i.e. the order differs between test participants.
This obviously leave us with the same concerns as in paper 1, that context has not thoroughly been investigated and taken
into consideration. As with the first paper, this was simply dismissed as a limitation and scope of this paper.

The experiment it self has also proved to be a problem even though it was thoroughly thought through. 
It consisted of four phases, we would use the first and last phase, and the second and third was for the other group. The phases were:

\begin{itemize}
\item 1. Usability test
\item 2. Waiting period (0min, 30min, 60min)
\item 3. Cued recall
\item 4. Cued recall debrief
\end{itemize}

The first phase is involves putting on sensors and doing the actual usability test. The second phase was having them wait for a period or time. 
The third phase involved reapplying the sensors and then having them watch a screen recording of them performing the usability test.
The fourth and last phase was a cued recall debrief session, where points of interests selected from visual inspection of GSR graphs was investigated with the participant and a researcher.

Our test was quite long, and the premisis of it being doable with the amount of participants we wanted, was that we could interleve different participants. 
When one participant was taking the usability test, another participant would be waiting for 30-60 minutes before comming back for cued recall. 
Unfortunately it turned out that it was impossible to hold the time plan. Partly because the test sometimes took longer than the wait time needed, and partly because the setup time varied a from person to person. Sometimes it would take 30 seconds to put on the EEG, sometimes it would take 10-20 minutes. This was unacceptable for the other group because their entire validity of the experiment was based around the wait time being exactly what it was meant to be.
This in turn meant we could only do 4-5 people a day, instead of 8 people a day. 
This along with the fact that sometimes people simply did not show up, meant that the testing period took significantly longer than anticipated. 
Further we also collected cued recall debrief data in the form of SAM questionnaires, however, it turned out those were not needed because the way they were collected did not fit the purposes we could use them for. We collected SAM for points of interest in a GSR graph, but what we really needed was to collect SAM data for each event/usability error or task, such that we could have validated our programs implemented errors actually also was perceived as usability errors. 

Specific to this test, the program developed is also a factor to consider when looking at the results.  We attempted to
make a program with no usability errors, but as research has shown since the dawn of HCI, this is nearly impossible.  We
experienced problems with the computer used for the test, where the program would have unexpected latency and unresponsiveness when using the keyboard and mouse. This was not intentional, and hence not a seeded usability problem.
The user might however, still perceive the unresponsiveness of the program as a usability problem. 
This was the case in at least one test.  
Further we found small areas of the program that could be considered cosmetic usability problems, however, as the paper does not actively try
to estimate severity this was left out in the evaluation. Other concerns became evident when considering how each
individual seeded problem was perceived by the test participant. Some problems were perceived immediately,
e.g. participants were paying attention when error feedback was given, while others problems were not. In particular,
tasks that required test participants to use the keyboard, usually draws their attention to it, and away from any
feedback signifying that an error has occurred. An implication that follows from this, is that it becomes harder to
determine exactly \textit{when} a test participant experiences a particular seeded problem.

The experiment could also be altered with the use of different hardware. It is possible to collect GSR data from the chest area, and heart rate data from the ears. This would free the dominant hand from sensors which would make the test participant use the system like normal, rather than having one hand disabled during the test.\\

\textit{Frustration models}\\
One of the biggest complications during this study is the lack of uniform agreement on how emotional responses, in particular frustration, developes on a physiological level. While there has been quite a few studies sorrounding estimating frustration from physiological data, the way they do it, and how they ``label'' frustration is very different.
The biggest difference lies in the assumption of the duration of the emotion. 
Some researchers, like us, believe the assumption that emotions are short lived and instantanious in nature. 
Others believe they are long, which can be anything from 10-20 seconds to over 100 seconds or more. 
While researchers generally tend to get good results, it has to do with the use case of the study. 
Do you look for a general increase in average amplitude of a GSR signal or do you look for actual spikes in amplitude? 
How you define this has a fundemental impact on the results you get. 
In our case it was difficult to select a model which satisfied all our constraints, especially because the experiment was designed prior the investigation of frustration as a physiological response. 
On one hand, we have a system which has ``events'', stimuli designed to be frustrating, within a task. These events can be exposed to the user in quick succession, depending on how fast the user provokes the event, e.g. clicking delete and nothing happens and doing it imediately again. 
A model which caters to the assumption that frustration lasts a long time, will conflict with the actual collected data, because it can overlap multiple events. Then it has to be considered if an outlier is caused by both events combined or an individual event. 

Further there is no uniform theory upon how frustrating events develop over time as there are multiple exposures. It is generally acknowledge that a reaction stimuli, positive or negative, is strongest the first time it is experienced. Further if a person can anticipate a stimuli, the expectation of it may also reduce the reaction. But is this the case with frustrating events in a running system as well? It is not too far stretched to imagine a person having a software related problem, and the first time you'll be slightly annoyed, if the problem persists the irritation that you cannot fix it also increase. That case could argue for a frustration curve which is steadily going upwards, but not having spikes.
On the other hand, if a person has written an entire document in word and it crashes, it will lead to a massive ``burst'' of frustration. 
While the reaction is labelled the same, the way it is experienced is very different. One is slow and steady, the other is instantanious and violent.

Due to the complexity of how to measure the data, and the resulting complex results of an explorative model also forced us to multiple times consider our hypothesis. Because the concept of frustration as physiological data has multiple layers of complexity, it is hard to boil it down to one number, or one result. Having it being a final number would simply be to simplistic a result compared to the model. This ruled out many of the initial hypothesis' revolving around significancy of the result of our classifiers. In the end we found the most fitting thing to do was not having a hypothesis, simply because the study is explorative of nature, and the result is insights of how researchers can deal with the complex nature of classifying frustration in a running program.\\

\textit{Novelty detection}\\
The method used in this paper is \textit{novelty detection} using a one-class SVM. The primary requirements for
achieving good performance, i.e. good predictive power, is to ensure that training data contains as few anomalies as
possible. The difficulty associated with ensuring this can vary depending on the kind of data considering, but in our
case, where we consider physiological data, is is not trivial. We attempt to minimize the risk of
training on data containing anomalies by considering only data from the first two tasks, containing no seeded
problems. However, we cannot be certain that this data never contains any anomalies, as identifying anomalies is what we
are attempting in the first place. This is to say, that our ability to identify anomalies is at most as good as the data
we train on. In order to ensure a better set of training data, it would be better to create a larger set of tasks, and verify those tasks and the software they are conducted in as being usability error free. 
The robustness of such a set of tasks would be better if such initial evaluation had been done.

A recurring concern, also present during the first paper and our 9th semester project, is that of finding the
\textit{correct} parameters and features for the classification algorithm. In an attempt to mitigate these concerns, we
performed grid searching, i.e. a near-exhaustive search, on parameters. However, doing so is considerably time consuming
and dependent on chosen features. This means, that although we can search for optimal parameters for a set of features,
a new search has to be performed if a new set of features are considered. This is to say, that validating if a set of
features yields good results, e.g. many true-positives and few false-positives, is considerably time consuming. Features
suggested in related work are disparate, i.e. varying time-spans and extracted from various statistics, and we have not
been able to find conclusive indications as to which features should be used. We have attempted to uncover differences
in features by considering two different sets of features - one considering short-term reactions and another considering
long-term reactions, governed by the time-spans considered. We present the findings under a discussion in the paper. We
want to make it clear, that finding optimal features and classification parameters, is not trivial. 
It is also debateable how robost the model is given we search for the optimal solutions. 
We have the \textit{best} model for a specific set of features, for a specific set of data. 
We cannot take this model and use it on another person. 

% write some shit about how we could have designed the test better


%remember to mention:
    %that training data has to be clean
    %specifying windows to specify as usability problems occurring
    %weighting individual sensors (what is a hit?)
    %scoring
    %features and time
    %svm params, grid search relies on the features selected
    %disparate opinions on time to consider for gsr

