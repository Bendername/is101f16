\section{Classification}
The use case of this article is to help third-party evaluators find usability problems in a usability test.
In a usability test the usability problems will occur an as an unknown class for the classifier, meaning that the classifier has model for the class that would be a usability problem. 
We therefore have to limit our choice of classifiers to the ones who can predict anomalies by only learning on the ´´´normal'' class.
Additionally the usability test is expected to have many normalities and only a small portion of anomalies.
This problem fits the scope of novelty detection\cite{noveltyDetection}.

There are many different method when working with novelty detection\cite{noveltyDetection}, and choosing the right one is not easy.
The different methods are divided into five subgroups probabilistic, distance based, domain based, reconstruction based, and information theoretic.
Manevitz et al.~\cite{oneClassSVM} showed that an One-class SVM achieved on average better results than neural networks, naive Bayes, nearest neighbor and prototype over a series of dataset.
Thus to predict whether or not a usability problem is present a one-class SVM is used.

\subsection{One-class SVM}
The one-class SVM is domain based an algorithm used for novelty detection, meaning it creates a boundary given its training data, and the unknown data is then labelled as a normality or anomaly regarding to its to position relative to the boundary.
Since the one-class SVM is very sensitive\cite{oneClassSVM} its parameter setting a a exhaustive grid search on the hyperparameters on the $Nu = \{0.01,0.06,..,0.41, 0.46\}$ and $Gamma = \{2^{-14},..2^2\}$ is performed, together with the Sigmoid and RBF kernel, to find the optimal parameters.
We use LibSVMSharp\cite{libsvmsharp}, a wrapper for LibSVM\cite{libsvm}, which is a library that have implemented a one-class SVM, since LIBSVM it is a widely used library for a SVM implementation.

\subsection{Prediction \& Scoring}
We create a one-class SVM for each of the sensors, each of the SVM's trains on the data from the first two tasks, which is design to contain no usability errors e.g. no anomalies. The model created can then be used to predict on remainder of the data.
A prediction from a one-class SVM is a binary 1 for a normality and -1 for an anomaly for the given a data point.
Which results in a collection of data points labelled with a 1 for normality or -1 for anomaly.

To decide whether an anomaly correctly hits an usability error some assumptions has to be made. Given our grouping of the task into instant error feedback and non instant error feedback, a split has to be made. The usability errors found in the instant error feedback tasks will be assigned to the specific time the error happened. Whereas for the non instant error feedback tasks will span from the first usability error related to that task to the last error related to that task has occurred, as illustrated in Figure x.x.
\todo{Make figure of this}
When an anomaly is found an area of 2.5 seconds to each side it is marked around is, to create a point of interest. The point of interest is created to satisfy the use case of a third-party evaluator having to look through the video, and thus the evaluator should have more than just a millisecond to see the problem.
To evaluate if an event is correctly found by the machine the two types of events will be considered again.
The events which contain instant feedback is classified as a hit the points of interest covers the time of which the event happens.
For the events which does not contain instant feedback, as mentioned before, an area for when the usability errors start to when the errors stops again is marked, if the point of interest hits inside the area the event will be marked as hit, both of these examples is illustrated in Figure x.x. \todo{Make Figure}

To find the best parameters for our exhaustive grid search a scoring function is needed 

Where $F_{1}$-score is usually defined as: \[F_1 = 2 * \frac{precision \times recall}{precision + recall}\] Where we have chosen to define our scoring function as: \[F_1 = 2 * \frac{precision \times EventsHitRate}{precision + EventsHitRate}\] \todo{Maybe rename precision}This is done to reward hitting as many different events as possible and not just the same event multiple times.


The fusion of the results from the different machines will done by a simple voting approach, where a machine must declare whether it has found an anomaly to the given point in time. If an certain amount of sensors agrees the given point will be flagged as an anomaly. 




