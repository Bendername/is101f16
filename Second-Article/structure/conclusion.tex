%!TEX root = ..\Main.tex
\section{Conclusion \& Discussion}
We found that on average, our classification did not yield results much better than random guessing. In general, Nu
parameters between 0.01 and 0.05 gave the best results, if few false-positives are desirable, resulting in at most
$7.8\%$ false coverage, but identifying $15.9\%$ UPs for GSR. Results deteriorate for the other sensors in this regard,
with EEG performing the worst. Choosing higher Nu-values quickly give rise to higher rates of false-positives, and
simultaneously higher event-hit rate. From this, we conclude that whether our results are good or not, depends on they
are applied. We imagine a use-case where few UPs are identified to a reasonable certainty, with a low Nu-value, used as
a preliminary usability test, before more elaborate usability testing is performed.

However, seeing aggregated and average results revealed surprising results for some test participants and sensors. From
Figures~\ref{fig:gsr_event_ehr,fig:eeg_event_ehr, fig:hr_event_ehr}, and \ref{fig:face_event_ehr}, it is apparent that
some test participants perform significantly better than the average. This lead to an investigation into how results
would look for only those. Choosing the five best performing test participants across all four sensors yielded results
seen in Figure~\ref{fig:best_five_gsr}. From Figure~\ref{fig:best_five_gsr}, we see significantly better average results
and individual results, compared to averages over all test participants. Compared with averages over all participants,
we see a large differences. While it may not be apparently surprising, we still think this raises interesting questions,
such as: are there circumstances that are equal among all five best-performing test participant? While we propose this
to be investigated in future work, we have some hypothesis of our own.

Because the tasks were not time framed, the data can be differently stretched, but with the same amount of
events. Looking at the available training data, the five best used an average of 2.8 minutes versus 4.1 minutes for the
five worst. This suggest that there is no difference in the predictive power against the training set between the worst
and the best. However, looking at the total test duration, minus the training tasks, the five best results used on
average 19 minutes versus the five worsts 13.8 minutes. This difference in time could mean that it is important to allow
enough unobtrusive experiences to occur between usability errors, such that the physiological responses return to
normal. In our test case, the events may actually happen too fast after each other, which could potentially mean some of
the false-positives or noise we get, are in fact true positives but just outside our assumption of the emotional
time-frame.

While there is a lot of litterature on this subject, few explore the development of frustration on a physiological level
over time.  Multiple studies attempts to classify frustration, however their usecases and context varies.  Some studies
considers a duration of 100 seconds or more\cite{machine_learning_100s_gsr}, while others considers durations of 15
seconds\cite{brainwave_signals_frustration}.  In this paper the most pessimistic option was selected; after a stimuli
there is a physiological delay followed by a measurable reaction which last a short duration.  This, however, implies
that frustration is not a continues reaction over time while using a system, but rather a reaction to a specific event
with a timelimit for how long it ``lasts in the body''.  However, such physiological reactions may in fact be unfolding
differently - frustration could be increasing in amplitude over time from the beginning of a stimuli.  That is, at the
start of a frustrating event a user may not produce physiological spikes large enough to be detectable with our current
setup, but over time as the frustrating event persists the physiological reaction will increase up until a point where
it is detectable.  This would lead to a situation where an anomaly may be detected X seconds from a stimuli, but in fact
the negative affective state started some time before that.  A more specific model for how such affective states unfold
and for how long will increase the performance of our, and similar setups, and a study into this is highly relevant.

Further it is also assumed that any anomaly within our test is caused by the system, and not an uncontrolled variable from outside the system. 
The reality is, however, that there is many areas which we cannot control in the experiment.
This could be noise from outside the test laboratory, hunger, or a stray thought of a family member passing away.
Naturally this can create noise, because a test participant might be shocked by the noise, or be in a more negative mood if hungry, which in turn potentially could impact how the test participant perceive the program. 
It could also be argued that the entire setup with sensors, and the type of test is not entirely compatible. 
In order to do a usability test, a user has to interact with a system of some kind. 
This almost always requires actions from the user, which in turn cause activity in the brain/muscles. 
This activity is unwanted in terms of the data collected from sensors, because it usually implies noise compared to the affective state changes which are wished to be collected. In order to truly make this setup compatible, it would require some filters which could remove this noise effectively.

We find the above largely unexplored, and as such leaves a lot of assumptions to be made.
This in turn creates a natural skepticism of the validity of our results.
While they may be valid within the assumptions and limitations of the experiment, they are not presented as a general model of detecting frustration. 
They are, however, a step in the right direction in terms of studying the affective state of a user while using a real-world system and another consideration to be made when trying to overcome the complex task of mapping affective states to physiological data. 
A natural path to explore would be removing the complexity of having sensor fusion, and instead focus on improving the understanding of how specific sensors captured data relates to affective states more generally. 
If such a model is ever conceived, it would be interesting to explore the idea of using ML to detect affective state
changes in greater detail.

The results of this paper does, however, propose some use if iterated further upon. 
It is possible to select a setting for the classifier, which finds usability problems with a fairly low amount of noise. 
This makes it possible to use when evaluating usability problems fast and with less effort than a traditional test. 
Simply use the system and hardware while conducting a test, and find few but significant usability errors. 
This could help filter away some errors early in the process, and focus on other problems when a larger more complete usability evaluation is conducted. 
This could be an additional tool for evaluators to add to the usability evaluation toolbox. 
This would most likely never remove the human aspect from evaluating the human experience of a system, but it would be a tool to look into the inner workings of a person, and help avoid some of the limitations related to usability evaluation.
In turn, this could reduce the time required to do usability tests and/or increase the effectiveness of an evaluation. 

\subsection{Future work}
One of the most promising areas to continue research within usability problem detection through physiological measured
affective state, is the means to detect the affective state change. One could imagine the potential in also being able
to detect if a user is having a \textit{pleasant} experience with a product.

We have chosen off-the-shelf standard and basic solutions in our attempt to solve the problem, but research aimed at optimizing these techniques should be done. 
In particular, ML techniques other than an 1-class SVM could be investigated, such as well as neural networks.

Our study has revolved around fusing multiple sensors, and while one method may work particularly well with a GSR, it may prove worse or even bad to use with an EEG. 
It could be interesting to investigate the results of various ML techniques across sensors.
Additionally, a further exploration into the feature space could also be conducted. 
Especially models such as DASM12\cite{eeg_music_listening} for the EEG, but would require better hardware.

Further research also has to be done with regards to the emotional ``baggage'' people arrive with. 
The personality type of a user change the way their physiological response is, such as the difference between introvert and extrovert GSR behavior~\cite{Foglia20081814}.
As such, it would be natural to hypothesize that other emotional and contextual influences such as being hungry or having ``a bad day'' may influence the physiological patterns as well. 
A thorough study to elaborate the considerations needed and consequences of such baggage would be beneficial to the field of study as a whole. 

Considering the five best performing test participants, we hypothesis that some similar circumstance may be present. We
find it interresting to investigate, as it could reveal important information into how to gather physiological data such
that it yields better results. We propose future work that investigates length of relaxation period, size of training
data among other things.
