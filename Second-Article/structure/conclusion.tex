%!TEX root = ..\Main.tex
\section{Conclusion}
We found that the average cases of our classification did not yield amazing results.
In general Nu parameters of 0.01 and 0.05 gave the best results, where the EHR on the GSR was 4.5\% and 14.5\% to 2.1\% and 7.2\% FCR respectively. Similar results were present for HR, but Kinect and EEG performed worse. 
Voting did not prove great either, with the best being conservative 2 voting which had 23.2\% EHR to 12.7\% FCR. 

\section{Discussion}
One of the most interesting things of our results, are that there are indications that our results suffer from bad worst-case tests. As the results are very different, it propose the idea that there exists different types of groups of people who react differently. If the best 5 test cases are looked at, we instead have much more promising results. The best five have ...\todo{write something in regards to the graphs we make}
The worst five, however, perform in the complete opposite way with a massive amount of noise \todo{write something about the worst graphs}.
Because the tasks were not time framed, the data can be differently stretched, but with the same amount of events. Looking at the available training data, the five best used an average of 2.8 minutes versus 4.1 minutes for the five worst. This suggest that there is no difference in the predictive power against the training set between the worst and the best. However, looking at the total test duration, minus the training tasks, the five best results used on average 19 minutes versus the five worsts 13.8 minutes. This difference in time could mean that it is important to allow enough unintrusive experiences to occur between usability errors, such that the physiological responses return to normal. In our test case, the events may actually happen to fast after each other, which could potentially mean some of the false-positives or noise we get, are in fact true positives but just outside our assumption of the emotional time-frame.

While there is a lot of litterature on this subject, few explore the development of frustration on a physiological level over time. 
Multiple studies attempts to classify frustration, however their usecases and context varies. 
Some studies considers a duration of 100 seconds or more\cite{machine_learning_100s_gsr}, while others considers durations of 15 seconds\cite{brainwave_signals_frustration}.
In this paper the most pessimistic option was selected; after a stimuli there is a physiological delay followed by a measurable reaction which last a short duration.
This, however, implies that frustration is not a continues reaction over time while using a system, but rather a reaction to a specific event with a timelimit for how long it ``lasts in the body''.
However, such physiological reactions may in fact be unfolding differently - frustration could be increasing in amplitude over time from the beginning of a stimuli. 
That is, at the start of a frustrating event a user may not produce physiological spikes large enough to be detectable with our current setup, but over time as the frustrating event persists the physiological reaction will increase up until a point where it is detectable. 
This would lead to a situation where an anomaly may be detected X seconds from a stimuli, but in fact the negative affective state started some time before that.
A more specific model for how such affective states unfold and for how long will increase the performance of our, and similar setups, and
a study into this is highly relevant.

Further it is also assumed that any anomaly within our test is caused by the system, and not an uncontrolled variable from outside the system. 
The reality, however, is that the brain, and physiological responses it creates, is highly sensitive according to the stimuli it receives.
This could be noise from outside the test laboratory, hunger, or a stray thought of a family member passing away. 
It could also be argued that the entire setup with sensors and the type of test is not entirely compatible. 
In order to do a usability test, a user has to interact with a system of some kind. 
This almost always requires actions from the user, which in turn cause activity in the brain/muscles. 
This activity is unwanted in terms of the data collected from sensors, because it usually implies noise compared to the affective state changes which are wished to be collected. In order to truly make this setup compatible, it would require some filters which could remove this noise effectively.

We find the above largely unexplored, and as such leaves a lot of assumptions to be made.
This in turn creates a natural skepticism of the validity of our results.
While they may be valid within the assumptions and limitations of the experiment, they are not presented as a general model of detecting frustration. 
They are, however, a step in the right direction in terms of studying the affective state of a user while using a real-world system and another consideration to be made when trying to overcome the complex task of mapping affective states to physiological data. 
A natural path to explore would be removing the complexity of having sensor fusion, and instead focus on improving the understanding of how specific sensors captured data relates to affective states more generally. 
If such a model is ever conceived, it would be interesting to explore the idea of using ML to detect affective state
changes in greater detail.

The results of this paper does, however, propose some use if iterated further upon. 
It is possible to select a setting for the classifier, which finds usability problems with a fairly low amount of noise. 
This makes it possible to use when evaluating usability problems fast and with less effort than a traditional test. 
Simply use the system and hardware while conducting a test, and find few but significant usability errors. 
This could help filter away some errors early in the process, and focus on other problems when a larger more complete usability evaluation is conducted. 
This could be an additional tool for evaluators to add to the usability evaluation toolbox. 
This would most likely never remove the human aspect from evaluating the human experience of a system, but it would be a tool to look into the inner workings of a person, and help avoid some of the limitations related to usability evaluation.
In turn, this could reduce the time required to do usability tests and/or increase the effectiveness of an evaluation. 

\subsection{Future work}
One of the most promising areas to continue research within usability problem detection through physiological measured
affective state, is the means to detect the affective state change. One could imagine the potential in also being able
to detect if a user is having a \textit{pleasant} experience with a product.

We have chosen off-the-shelf standard and basic solutions in our attempt to solve the problem, but research aimed at optimizing these techniques should be done. 
In particular, ML techniques other than an 1-class SVM could be investigated, such as well as neural networks.

Our study has revolved around fusing multiple sensors, and while one method may work particularly well with a GSR, it may prove worse or even bad to use with an EEG. 
It could be interesting to investigate the results of various ML techniques across sensors.
Additionally, a further exploration into the feature space could also be conducted. 
Especially models such as DASM12\cite{eeg_music_listening} for the EEG, but would require better hardware.

Further research also has to be done with regards to the emotional ``baggage'' people arrive with. 
The personality type of a user change the way their physiological response is, such as the difference between introvert and extrovert GSR behavior~\cite{Foglia20081814}.
As such, it would be natural to hypothesize that other emotional and contextual influences such as being hungry or having ``a bad day'' may influence the physiological patterns as well. 
A thorough study to elaborate the considerations needed and consequences of such baggage would be beneficial to the field of study as a whole. 