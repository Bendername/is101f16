%!TEX root = ..\Main.tex
\section{Conclusion \& Discussion}
HR, Kinect, EEG and GSR were all tested individually across different Nu values, results showed that EEG performed the worst but the GSR and HR showed encouraging results. It was also establish that choosing a higher Nu-values grants a higher EHR and FCR, and from Figure~\ref{fig:gsr_event_ehr}, \ref{fig:eeg_event_ehr}, \ref{fig:hr_event_ehr} and \ref{fig:face_event_ehr} it can be seen that a good ratio of EHR and FCR are kept showing good robustness of the machines.
We imagine a use-case where few usability problem are identified to a reasonable certainty, with a low Nu-value, used as
a preliminary usability test, before more elaborate usability testing is performed. It can also be used to choose a higher Nu-value to give a higher EHR but this comes with the trade-off of getting more FCR.

From  Figures~\ref{fig:gsr_event_ehr},\ref{fig:hr_event_ehr}, and \ref{fig:face_event_ehr}, it is apparent that
some test participants perform better than the average. This lead to an investigation into how results
would look for only those. Choosing the five best performing test participants across all four sensors yielded results
seen in Figure~\ref{fig:best_five_gsr}. From Figure~\ref{fig:best_five_gsr}, we see significantly better average results
and individual results, compared to averages over all test participants. Compared with averages over all participants,
we see a large differences. While it may not be apparently surprising, we still found it a interesting point of focus.
By looking at the best five test participants on the GSR, we found that they in average had a lower value on the introvert/extrovert category in Big5.

Because the tasks were not time framed, the data can be differently stretched, but with the same amount of
events. Looking at the available training data, the five best used an average of 2.8 minutes versus 4.1 minutes for the
five worst. This suggest that there is no difference in the predictive power against the training set between the worst
and the best. However, looking at the total test duration, minus the training tasks, the five best results used on
average 19 minutes versus the five worsts 13.8 minutes. This difference in time could mean that it is important to allow
enough unobtrusive experiences to occur between usability errors, such that the physiological responses return to
normal. In our test case, the events may actually happen too fast after each other, which could potentially mean some of
the false-positives or noise we get, are in fact true positives but just outside our assumption of the emotional
time-frame.

While there is a lot of literature on this subject, few explore the development of frustration on a physiological level
over time.  Multiple studies attempts to classify frustration, however their use cases and context varies.  Some studies
considers a duration of 100 seconds or more\cite{machine_learning_100s_gsr}, while others considers durations of 15
seconds\cite{brainwave_signals_frustration}.  In this paper the most pessimistic option was selected; after a stimuli
there is a physiological delay followed by a measurable reaction which last a short duration.  This, however, implies
that frustration is not a continues reaction over time while using a system, but rather a reaction to a specific event
with a time limit for how long it ``lasts in the body''.  However, such physiological reactions may in fact be unfolding
differently - frustration could be increasing in amplitude over time from the beginning of a stimuli.  That is, at the
start of a frustrating event a user may not produce physiological spikes large enough to be detectable with our current
setup, but over time as the frustrating event persists the physiological reaction will increase up until a point where
it is detectable.  This would lead to a situation where an anomaly may be detected X seconds from a stimuli, but in fact
the negative affective state started some time before that.  A more specific model for how such affective states unfold
and for how long will increase the performance of our, and similar setups, and a study into this is highly relevant.

Further it is also assumed that any anomaly within our test is caused by the system, and not an uncontrolled variable from outside the system. 
The reality is, however, that there is many areas which we cannot control in the experiment.
This could be noise from outside the test laboratory, hunger, or a stray thought of a family member passing away.
Naturally this can create noise, because a test participant might be shocked by the noise, or be in a more negative mood if hungry, which in turn potentially could impact how the test participant perceive the program. 
It could also be argued that the entire setup with sensors, and the type of test is not entirely compatible. 
In order to do a usability test, a user has to interact with a system of some kind. 
This almost always requires actions from the user, which in turn cause activity in the brain/muscles. 
This activity is unwanted in terms of the data collected from sensors, because it usually implies noise compared to the affective state changes which are wished to be collected. 
In order to truly make this setup compatible, it would require some filters which could remove this noise effectively.
The question is, if it is ever truly going to be controllable, and in turn if it will ever be feasible enough to measure the affective state in the resolution that physiological data provide.
The more resolution, the more of the unwanted data is going to be present and have to be dealt with.

We find the above largely unexplored, and as such leaves a lot of assumptions to be made.
This in turn creates a natural skepticism of the validity of our results.
While they may be valid within the assumptions and limitations of the experiment, they are not presented as a general model of detecting frustration. 
They are, however, a step in the right direction in terms of studying the affective state of a user while using a real-world system and another consideration to be made when trying to overcome the complex task of mapping affective states to physiological data. 
A natural path to explore would be removing the complexity of having sensor fusion, and instead focus on improving the understanding of how specific sensors captured data relates to affective states more generally. 
If such a model is ever conceived, it would be interesting to explore the idea of using ML to detect affective state
changes in greater detail.

The results of this paper does, however, propose some use of the result found in this paper.
It is possible to select a setting for the classifier, which detect a few amount usability problems with a fairly low amount of noise, which can be used to evaluating usability problems fast and with less effort than a traditional test. With the trade-off of only finding a few of the problems.
It would be interesting to examine if this still hold true in a large test with only a few usability problems.
This could help filter away some errors early in the process, and focus on other problems when a larger more complete usability evaluation is conducted. 
The method in general would most likely never remove the human aspect from detecting usability problem in a system, but it would be a tool to look into the inner workings of a person, and help avoid some of the limitations related to usability evaluation.

\subsection{Future work}
One of the most promising areas to continue research within usability problem detection through physiological measured
affective state, is the means to detect the affective state change. One could imagine the potential in also being able
to detect if a user is having a \textit{pleasant} experience with a product.

Our study has revolved around fusing multiple sensors, and while one method may work particularly well with a GSR, it may prove worse or even bad to use with an EEG. 
It could be interesting to investigate the results of various ML techniques across sensors.
Additionally, a further exploration into the feature space could also be conducted. 
Especially models such as DASM12\cite{eeg_music_listening} for the EEG, but would require better hardware.

Further research also has to be done with regards to the emotional ``baggage'' people arrive with. 
The personality type of a user change the way their physiological response is, such as the difference between introvert and extrovert GSR behavior~\cite{Foglia20081814}.
As such, it would be natural to hypothesize that other emotional and contextual influences such as being hungry or having ``a bad day'' may influence the physiological patterns as well. 
A thorough study to elaborate the considerations needed and consequences of such baggage would be beneficial to the field of study as a whole. 

Considering the five best performing test participants, we hypothesis that some similar circumstance may be present. We
find it interesting to investigate even further, as it could reveal important information into how to gather physiological data such
that it yields better results. We propose future work that investigates length of relaxation period, size of training
data among other things.
