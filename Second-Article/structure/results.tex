%!TEX root = ..\Main.tex
\section{Results}
The mean length of each test, excluding the two first tasks used for training, were $11.8$ minutes, with a standard
deviation of $4.4$ minutes - the longest being $28.3$ minutes and the shortest $5.8$ minutes.

Our system produces on average $17$ events from test participants, within which our setup is expected to predict
usability problems, and standard deviation of $4$. One participant produced the most with $26$ and the least produced
was $11$,

The ratio between area designated to containing UPs, referred to as events, and areas that do not contain UPs, in on
average $0.39$, i.e. a little more than 60\% of the data on which we attempt to predict UPs is \textit{normal} data. The
implication of this is that if a classifier were to distribute random guesses based on our data, about 60\% of them
would fall within normal data, and 40\% within data that contains UPs, slightly shifting the difficulty to our
disadvantage. The standard deviation in this ratio is $0.06$, and in only one case, is the area containing UPs larger
with a ration of $0.58$. The smallest example showed $0.25$, i.e. 25\%, of the test consisting of events containing
usability problems.

\input{structure/sensors_graphs}

Having performed a grid search to find optimal settings for our one-class SVM classification, we performed the mentioned
Nu-value line search. Figures~\ref{fig:gsr_event_ehr,fig:eeg_event_ehr, fig:hr_event_ehr}, and \ref{fig:face_event_ehr}
shows the result of this as scatter plots of EHR and FCR for each sensor. Nu values range from low/green to high/red,
ranging from $0.01$ to $1.00$ in $0.01$ intervals, yielding 100 different settings and results for each test
participant. All test participants are represented in each graph, and Nu-values with a thick border is the average of
each Nu-value across all test participants.

\paragraph{Sensors likeness and differences}
\input{structure/avg_stats}
As seen in Figure~\ref{fig:face_event_ehr} and Table~\ref{[TABLE]_avg_stats_sensors}, the Kinect shows precision above random guessing at low Nu-values and,
as expected, closes in on the random guessing (37,52\%), as we increase the Nu-value. However, as seen in Table
\ref{[TABLE]_avg_stats_sensors}, the GSR only slightly excite the random ratio at some of the values. Meaning that
to have a reasonable precision with few false positives, a lower Nu-value would be desirable.
These findings indicate that the results from the Kinect are presumably not related to random guessing, but can give a moderately qualified answer to where usability problems can be found, and the GSR show some of the same tendencies.
While the HR values does not excite the threshold for random guessing, it still shows encouraging results in regards of hitting as many different events as possible while keeping a low FCR, as seen in Figure~\ref{fig:hr_event_ehr}.

Looking at Figures~\ref{fig:gsr_event_ehr}, \ref{fig:eeg_event_ehr}, \ref{fig:hr_event_ehr} and \ref{fig:face_event_ehr},
as well as Table~\ref{[TABLE]_avg_stats_sensors}, it can be seen that choosing a higher Nu-value for your classifier can yield interesting propositions if the classifier should cover as many problems as possible while minimizing the area wrongly covered.
In other words if one's aim is to hit all events, a high Nu-value must be chosen, but comes with the trade-off of placing more anomalies outside events. 
While the GSR and the HR both have a smooth curve through the averages of which indicates a stable classifier, but the Kinect seem to be more unstable in its relation between EHR and FCR. However Kinect seems to regain some of its stability with higher Nu values. As shown in \ref{[TABLE]_avg_stats_sensors} and seen in Figure x\todo{THIS NEEDS REF}, the EEG deviate from the other by already at low Nu-values doing a very aggressive prediction and predicting x amount more in average than the X sensor.

All the graph reveals that across all the test participants no golden Nu-value presents itself. A ``conservative''
setting could be chosen, to ensure that little false area is covered, while still detecting some usability problems. On
the other hand, a higher and more ``aggressive'' value could be chosen in attempt to detect as many problems as
possible, with the trade-off of alse receiving many false-positives. Detecting usability problems \textit{is} possible,
however some sensors approach, on average, a precision very close to guessing.  Figures~\ref{fig:gsr_event_ehr},
\ref{fig:eeg_event_ehr}, \ref{fig:hr_event_ehr} and \ref{fig:face_event_ehr} and Table~\ref{[TABLE]_avg_stats_sensors}
still suggest that even though precision is low, it still manages to have a good EHR given the FCR.

\paragraph{Investigating extremes}
Examining Figures~\ref{fig:gsr_event_ehr}, \ref{fig:eeg_event_ehr}, \ref{fig:hr_event_ehr} and \ref{fig:face_event_ehr},
we see a substatial density of datapoints above average. We find it interresting to consider the best performing test
participants. Doing so, enables us to discuss how our setup might perform under better circumstances.

\input{structure/best_five_graphs}
\input{structure/best_five_stats}

We decided to choose the five best performing test participants, and plot their results as seen in
Figure~\ref{fig:best_five_gsr}. Tabel~\ref{tab:best_five_avg_stats} shows average statistics for both the five best, and
the best performing across all test participants.

From Figure~\ref{fig:best_five_gsr} we see that physiological data for some test participants generate better results
than others. We do not believe this to be a coincidence, however futher work has to be conducted in order to confirm this.