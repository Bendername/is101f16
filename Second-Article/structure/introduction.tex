%!TEX root = ..\Main.tex
	
\section{Introduction}
%http://delivery.acm.org.zorac.aub.aau.dk/10.1145/2500000/2491883/p258-liapis.pdf?ip=130.225.53.20&id=2491883&acc=ACTIVE%20SERVICE&key=36332CD97FA87885%2E1DDFD8390336D738%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&CFID=764220047&CFTOKEN=51730471&__acm__=1459164823_6fcb563aae9287be26350addc8efbe52
%artikel vi kan bruge som st√∏tte.

Usability testing has long been an important aspect of software development.
However, as Bruun et al.~\cite{LH-paper} mention, the approaches used to
identify usability problems (UP) and categorise their severity are still based
on traditional methods and performance based metrics such as task completion and effort required. \cite{LH-paper} suggests revised methods
for identifying UPs that are instead based on a user's affective state as
traditional methods are encumbered by several fallacies.
%[17, 34, 60, 77] = "UP defined in terms of cognetive and performance-based
%impacts that the UP excerts on the user or on developers"

% maybe mention here that our work can be considered an extension/future work to
% Anders' work (we share his sentiment/agree with him)

Traditional methods for identifying UPs focus on the time taken to complete a
given task and the effort required. Strategies such as \textit{think
aloud}~\cite{use_of_TA_and_IDA} can help observers better gauge a user's
thoughts, and to some extent affective state, while experiencing UPs, but does
not formalise capturing affective state in particular and is inherently a third-party
subjective evaluation. Three predominant methods formalised for capturing a
user's affective state are affective self-report, physiological reactivity and
observable behaviour~\cite{BRADLEY199449}. Strategies such as \textit{think
aloud} fall within the \textit{observable behaviour} category, while
\textit{self-report} covers methods such as \textit{Self-Assessment Manikin}
~\cite{BRADLEY199449}, both of which suffers from 
subjectivity-bias. Observable behaviors inherently because third-party evaluators are making a subjective opinion based of off their own estimations and analysis' which can vary\cite{eval_effect}, and SAM because of effects like the peak-end memory bias \cite{cockburn_peakend}. Physiological reactivity requires measuring a
user's physiology during testing and is not yet regarded as common practice\cite{9th_semester_project}.  Physiological measuring can help reduce or
eliminate the subjectivity-bias in traditional methods, and we hypothesise that
the reason it has not yet reached wide-spread adoption is due to the lack of
reliable and formalized methods as well as the accuracy of consumer grade hardware.

Bruun et al.~\cite{LH-paper} made a usability case-study of ``Danmarks Statistik (DST)''.
A number of UP's were found in an independent emperical study, for which they created a main task with three sub tasks for participants to complete.
They recorded physiological data using a \textit{galvanic skin response} (GSR) which measures sweat \cite{gsr_calibration}, and an eye-tracker detecting the gaze of the participant.
They formalized a method and a formula for associating the physiological data with the discrete negative affect state \textit{frustration}.
As mentioned in multiple studies, e.g. ~\cite{LH-paper}, ~\cite{frustration_with_computers}, frustration is well-studied
and manifests whenever expectations or rewards are not met in a timely manner or conflict when a goal is compromised in one way or another.
They had a cued-recall debrief session where the participants reviewed video clips found from GSR peaks, and filled out SAM scales relating to the clip. 
Bruun et al. found a correlation between peaks found in GSR signals and SAM-ratings, but was unable to confirm a relationship between the
\textit{severity} of a UP and the level of frustration experienced.

While Bruun et al. found a correlation, the entire study still relies on the preliminary assumption that the UPs found by third-party evaluations on DST are correct. 
As mentioned such evaluations suffers from various problems, and are generally only considered to find a subset of the UPs present in a system. 
With recent advances in consumer graded hardware and machine intelligence, and the fact that studies show correlations between UPs and measured physiological responses, we propose another approach where the usage of sensors and physiological data could be used to find UPs.
This could potentially elimate some of the need for third-party expert evaluations and not only save time, but also mitigate implications such as the \textit{evaluator effect}\cite{eval_effect}.
We define the preliminary research question as:\\

\textbf{RQ}. \textit{Is it possible to use physiological data from consumer graded hardware and machine learning, to
  elimate some of the implications from third-party evaluations, and save time in usability evaluations?}

% TODO: write more to intro
    %- Make link to our first article (that we can measure affective state to
%some degree based in stimuli induced by images (IAPS), using multiple sensors
%and fusing their results)
    %- Mention other caveat (multifaceted emotions, non-orthogonal A/V, that
        %Anders only measure arousal etc., that we aim to solve
    %- State briefly how we setup a test-environment (seeded problems, sensors, cued-recall debrief) to try and confirm/reach our goal
    %- Argue that a subjective measurement of affective state could considerably reduce UT complexity and errors
    %    - Argue how this is the case

\section{Related work}
This paper focuses on three different areas in the HCI context. (1) Methods used for usability- and user experience-testing. (2)General use of sensors in HCI. (3) The use of sensors in usability- and user experience-testing.

Several attempts have been made to improve both efficiency and accuracy of
usability evaluation experiments. Kjeldskov and Stages' \textit{Instant Data
Analysis} (IDA) method aims to significantly reduce the effort and time required
to post-analyse data when identifying usability
problems~\cite{instant_data_analysis}. Compared to traditional video data
analysis techniques, they found that in only 10\% time they were able to
identify 85\% of critical usability problems. Jonathan J. et
al.~\cite{use_of_TA_and_IDA} found the IDA
method compelling for its reduction in time and effort required, but mentions
several shortcomings to take into consideration of using it over a traditional
method.

\textit{Think-Aloud} sessions can help evaluators extract the thought-process
and affective state of test participant while they interact with the system
under test. During think-aloud sessions, the test participant verbalizes how
they interact with the system and how they feel while doing so.  Several studies
have been made, investigating the efficiency and accuracy of think-aloud
\textit{protocols}, i.e. how the subject should be interrogated during
testing~\cite{two_think_aloud_protocols_study}. However, no matter how refined
such methods become, they are inherently subjective.

While still a relatively new and unexplored area within HCI research,
physiological sensors are increasingly being applied in usability experiments.
Elling et at.~\cite{concurrent_think_aloud_eye_tracking} investigated the
relationship between what users verbalized during think-aloud sessions and where
their gaze was at (i.e. used eye-tracking equipment), to both scrutinize the
think-aloud method and to complement it. Similarly, P\"{a}tsch et
al.~\cite{using_sensor_graphs_think_aloud} complimented think-aloud recall
sessions using GSR sensor data.

A phenomenon that can significantly impact how and which usability problems are
found and categorized, is the \textit{evaluator effect}~\cite{eval_effect}.
The evaluator effect is the phenomenon that a set of evaluators will individually only find a subset of all usability problems.
Further the evaluators will tend to find different problems, and score them differently as well.\cite{eval_effect}
Hertzum et al. found that not only do
usability experts identify substantially different usability problems, they also
disagree on how they should be categorized in
severity~\cite{eval_effect_research}. Based on their findings, they suggest that
several evaluators and domain experts should participate in evaluating critical
software.

As mentioned in the introduction, Bruun et al. combined both GSR sensor and
eye-tracking data to compliment think-aloud recall sessions. They found a
significant correlation between moments of peaks in GSR readings and instances
of usability problems. They results were however encumbered by the
\textit{evaluator effect} and conclude that a structured and operationalized
method for capturing affective state and associated it with usabilty problem
severity is still needed within HCI.

It is clear from the literature and recent research trends within the HCI community, that a revision of the traditional usability evaluation methods is being explored. \todo{er det clear though?}
Current methods rely on ''experts' and their subjective analytic abilities whilst being focused on performance-based metrics.
As exemplified, more research is taking place within the area of using and applying sensors to estimate the affective state of users, however, this research is limited and usually only one sensor is used. 
Another complication is the fact that much of the hardware is expensive, possibly because consumer graded hardware is too imprecise to yield robust results.

The focus of this paper is to use multiple consumer graded hardware sensors to increase the overall robustness of the system as a whole. 
By using multiple sensors and appropriate machine intelligence techniques we hope to show that it is possible to use objective physiological data to accurately find usability problems as a direct alternative or extra tool to traditional usability evaluation methods. 
As such, our hypothesis' are:\\\\

\newcommand{\hypo}[2]{%
  \textbf{H#1:} \textit{#2} \\
}
%\textit{\textbf{H.1.} Physiological sensor data can be used to detect usability problems with a significantly low amount of false-positives. False-positives are the classification of a usability problem being present, while it is not.}\\\\
%\textit{\textbf{H.2.} Physiological sensor data can be used to detect usability problems with
% a significantly low amount of false-negatives. False-negatives are the failure to classify a usability problem being present}\\\\
%\textit{\textbf{H.3.} There is a statistical significant correlation between severity ratings and physiological measurements.}

\hypo{1}{Using consumer-graded physiological sensor-hardware, it is possible with a significant degree
  of certainty\todo{Tror ikke det er muligt at sige her, med mindre vi definere hvad vi skal s√¶tte det op i mod}, to predict moments where usability problems are experienced during a usability test. }

\hypo{1a}{Using physiological data from multiple sensors increase the degree of certainty to which moments of usability problems are predicted}

\hypo{1b}{Using physiological data from multiple sensors decrease the likelihood to which false-positives occur.}\todo{Kan vi godt pr√∏ve at se p√•, men vi skal have defineret hvad vi mener med en false positiv}

\hypo{2}{Usability problems of higher severity are more likely to be correctly predicted compared to
moments containing usability problems of lower severity}

%http://www.tandfonline.com.zorac.aub.aau.dk/doi/pdf/10.1080/03610730500206808
% Cite for SAM

%http://www.robots.ox.ac.uk/~davidc/pubs/NDreview2014.pdf
% Cite and use for novelty detection

Predictions are made post-test on the physiological sensor data
 False-positives are moments predicted to contain usability problems, but where no usability problems were experienced
