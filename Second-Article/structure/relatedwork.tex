\section{Related work}
This paper focuses on three different areas in the human-computer interaction(HCI) context. (1) General methods used for usability testing and user experience evaluation. (2) General use of sensors in HCI. (3) The use of sensors in usability- and user experience-testing.

\subsection{Traditional methods} %Jess soro, snakker om irritation i de forskellige kategorier.


Several attempts have been made to improve both efficiency and accuracy of
usability testing. Kjeldskov and Stages' \textit{Instant Data
Analysis} (IDA) method aims to significantly reduce the effort and time required
to post-analyse data when identifying usability
problems~\cite{instant_data_analysis}. Compared to traditional video data
analysis techniques, they found that in only 10\% time they were able to
identify 85\% of critical usability problems. Jonathan J. et
al.~\cite{use_of_TA_and_IDA} found the IDA
method compelling for its reduction in time and effort required, but mentions
several shortcomings to take into consideration of using it over a traditional
method.

\textit{Think-Aloud} sessions can help evaluators extract the thought-process
and affective state of test participant while they interact with the system
under test. During think-aloud sessions, the test participant verbalizes how
they interact with the system and how they feel while doing so.  Several studies
have been made, investigating the efficiency and accuracy of think-aloud
\textit{protocols}, i.e. how the subject should be interrogated during
testing~\cite{two_think_aloud_protocols_study}. However, no matter how refined
such methods become, they are inherently subjective.

A phenomenon that can significantly impact how and which usability problems are
found and categorized, is the \textit{evaluator effect}~\cite{eval_effect}.
The evaluator effect is the phenomenon that a set of evaluators will individually only find a subset of all usability problems.
Further the evaluators will tend to find unrelated problems from one another, and score them differently as well.\cite{eval_effect}
Hertzum et al. found that not only do
usability experts identify substantially different usability problems, they also
disagree on how they should be categorized in
severity~\cite{eval_effect_research}. Based on their findings, they suggest that
several evaluators and domain experts should participate in evaluating critical
software.

Identifying usability problems on a physiological, or psychological level, has been shown to be just as difficult. In a
shift towards a focus on \textit{user experience}, several attempts\todo{What several attempts?} have been made at identifying usability problems
from the perspective of how test participants are affected by the experience of interacting with a system. In other
words, the affective state of a test participant might reveal potential problems within a software system. Jussi
P.P. Jokinen~\cite{workplace_up_study} investigated how frustration plays a key-role in how individuals interact with a
system. Likewise, Lazar et al.~\cite{frustration_with_computers} found that text-processing and email-related tasks
induces the highest amount of frustration.

\subsection{Sensors and usability testing}
Sensors has been used in various studies in the HCI field, serving as input devices to detecting the well being of a person.
Moreover some of these studies focus is to try and predict a test participants emotions or affective state in different test scenarios. 
An example could be Schmidt et al.~\cite{schmidt_trainor}, who used an electroencephalogram (EEG) to identify valence and intensity, where they found some correlation with different part of the brain.
Zhai et al.~\cite{gsr_data_processing2} used galvanic skin response (GSR), blood volume pulse (BVP), pupil diameter (PD) and skin temperature to measure if a person was stressed or not stressed. Where they achieved a 90\% accuracy with an support vector machcine (SVM) and 11 features.

While still a relatively unexplored area within HCI research,
physiological sensors are increasingly being applied in usability experiments.
Elling et at.~\cite{concurrent_think_aloud_eye_tracking} investigated the
relationship between what users verbalized during think-aloud sessions and where
their gaze was at (i.e. used eye-tracking equipment), to both scrutinize the
think-aloud method and to complement it. Similarly, P\"{a}tsch et
al.~\cite{using_sensor_graphs_think_aloud} complimented think-aloud recall
sessions using GSR sensor data.

In 2014 Liapis et. al~\cite{fusion4} has created a PhysiOBS a post-test data processing tool to help usability test evaluators speed up the analysis process of a usability test. It do this by giving the evaluator the ability to label specific areas with emotions, showing both screen and face camera in one window, as well as physiological data collected from different sensors. It also offers indicators for what task users is doing at the given time.

Aggarwal et al.~\cite{sensor_example} used an Emotiv EPOC and their Emotivs build-in software to detect frustration in a user experience test with three seeded events. Where they show to some promising results.

From the literature and recent research trends within the HCI community, that a revision of the traditional usability evaluation methods is being explored to ease the task of analysing the results from a test. Current methods mostly rely on experts and their subjective analytic abilities whilst being focused on performance-based metrics.
Furthermore some attempts, using sensors, has been made to automate the process of analysing the data from the usability test or user experience evaluation. 