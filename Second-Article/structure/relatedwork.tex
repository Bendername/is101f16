%!TEX root = ..\Main.tex
\section{Related work}
The related work explored in this paper focuses on two different areas in the field of human-computer interaction(HCI). (1) Traditional methods used for usability testing and user experience evaluation. (2) Then an exploration of the use of sensors in HCI, after which it further details the use of sensors in a usability testing domain.

\subsection{Traditional methods}
Several attempts have been made to improve both efficiency and the accuracy of
usability testing. Kjeldskov and Stage's \textit{Instant Data
Analysis} (IDA) method aims to significantly reduce the effort and time required
during post-analysis of data when identifying usability
problems~\cite{instant_data_analysis}. Compared to traditional video data
analysis techniques, they found that in only 10\% or the normal time used, they were able to
identify 85\% of critical usability problems. Jonathan J. et
al.~\cite{use_of_TA_and_IDA} found the IDA
method compelling for its reduction in time and effort required, but mentions
several shortcomings to take into consideration of using it over a traditional
method, in particular the still-present evaluator effect.

The ``evaluator effect'' can significantly impact how and which usability problems are
found and categorized~\cite{eval_effect}.
The evaluator effect is the phenomenon that a set of evaluators will individually only find a subset of all usability problems.
Hertzum et al.~\cite{eval_effect_research} found that not only do
usability experts identify substantially different usability problems, they also
disagree on how they should be categorized in
severity. Based on their findings, Hertzum et al. suggested that
several evaluators and domain experts should participate in evaluating critical
software.

\textit{Think-Aloud} sessions can help evaluators extract the thought-process
and affective state of test participant while they interact with the system
during a test. In think-aloud sessions, the test participant verbalizes how
they interact with the system and how they feel while doing so.  Several studies
have been made, investigating the efficiency and accuracy of think-aloud
\textit{protocols}, i.e. how the subject should be interrogated during
testing~\cite{two_think_aloud_protocols_study}. However, no matter how refined
such methods become, they are inherently subjective.

Identifying usability problems on a physiological, or psychological level, has been shown to be just as difficult. In a
shift towards a focus on \textit{user experience}, attempts are made to identify usability problems
from the perspective of how test participants are affected by the experience of interacting with a system. In other
words, the affective state of a test participant might reveal potential problems within a software system. Jussi
P.P. Jokinen~\cite{workplace_up_study} investigated how frustration plays a key-role in how individuals interact with a
system. Likewise, Lazar et al.~\cite{frustration_with_computers} found that text-processing and email-related tasks
induce the highest amount of frustration for a user in a work environment. Lastly, Jeff Sauro suggests a revised usability problem severity scale based on how
\textit{irritated} test participants become~\cite{jeff_severity}.

\subsection{Sensors and usability testing}
Sensors have been used in various studies within the field of HCI, serving as input devices to detecting the well-being of a person.
Moreover some studies try and predict a test participants emotions or affective state in different test scenarios. 
An example could be Schmidt et al.~\cite{schmidt_trainor}, who used an electroencephalogram (EEG) to identify valence and intensity, where they found some correlation with different parts of the brain.
And Zhai et al.~\cite{gsr_data_processing2} used GSR, blood volume pulse (BVP), pupil diameter (PD) and skin temperature to measure if a person was stressed or not, where they achieved a 90\% accuracy with a support vector machine (SVM) and 11 features.

While still a relatively unexplored area within HCI research,
physiological sensors are increasingly being applied in usability experiments.
Elling et al.~\cite{concurrent_think_aloud_eye_tracking} investigated the
relationship between what users verbalized during think-aloud sessions and where
their gaze was at (i.e. used eye-tracking equipment), to both scrutinize the
think-aloud method and to complement it. Similarly, P\"{a}tsch et
al.~\cite{using_sensor_graphs_think_aloud} complimented think-aloud recall
sessions using GSR sensor data.

In 2003, Ward et al.\cite{shitty_web} made two different designs of the same website, one that followed best practices, and one which tried to break them. 
They had 20 test participants do tasks for 10 minutes, the first minute was used for a baseline GSR and HR reading which the remaining 9 minutes were compared to. They looked at changes in skin conductivity, heart rate beats per minute and finger blood volume.
They compared the ill-designed website results and the proper designed website results.
They found an increase in both heart rate beats per minute and skin conductivity, but not in finger blood volume.

In 2014 Liapis et. al~\cite{fusion4} created PhysiOBS, a post-test data processing tool to help usability test evaluators speed up the analysis process of a usability test. It does this by giving the evaluator the ability to label specific areas with emotions, showing both screen and face camera in one window, as well as physiological data collected from different sensors. It also offers indicators for what task users is doing at the given time.
Aggarwal et al.~\cite{sensor_example} used an Emotiv EPOC(EEG) and the vendor-supplied software to detect frustration in a
user experience test with three seeded events. While they do not present conclusive results, they claim to be able to
pinpoint moments of frustration experience by test participants during usability testing.

It is evident from recent research and trends within the HCI community, that revisions of the traditional usability
evaluation methods are being explored to ease the task of analyzing the results from a test.
Current methods mostly rely on experts and their subjective analytic abilities with a strong emphasis on performance-based
metrics, and state-of-the-art research is pushing for more objective and user-centric analysis.