
\section{Related work}
This paper focuses on three different areas in the HCI context. (1) Methods used for usability- and user experience-testing. (2)General use of sensors in HCI. (3) The use of sensors in usability- and user experience-testing.

\subsection{Common methods}
Several attempts have been made to improve both efficiency and accuracy of
usability evaluation experiments. Kjeldskov and Stages' \textit{Instant Data
Analysis} (IDA) method aims to significantly reduce the effort and time required
to post-analyse data when identifying usability
problems~\cite{instant_data_analysis}. Compared to traditional video data
analysis techniques, they found that in only 10\% time they were able to
identify 85\% of critical usability problems. Jonathan J. et
al.~\cite{use_of_TA_and_IDA} found the IDA
method compelling for its reduction in time and effort required, but mentions
several shortcomings to take into consideration of using it over a traditional
method.

\textit{Think-Aloud} sessions can help evaluators extract the thought-process
and affective state of test participant while they interact with the system
under test. During think-aloud sessions, the test participant verbalizes how
they interact with the system and how they feel while doing so.  Several studies
have been made, investigating the efficiency and accuracy of think-aloud
\textit{protocols}, i.e. how the subject should be interrogated during
testing~\cite{two_think_aloud_protocols_study}. However, no matter how refined
such methods become, they are inherently subjective.

A phenomenon that can significantly impact how and which usability problems are
found and categorized, is the \textit{evaluator effect}~\cite{eval_effect}.
The evaluator effect is the phenomenon that a set of evaluators will individually only find a subset of all usability problems.
Further the evaluators will tend to find unrelated problems from one another, and score them differently as well.\cite{eval_effect}
Hertzum et al. found that not only do
usability experts identify substantially different usability problems, they also
disagree on how they should be categorized in
severity~\cite{eval_effect_research}. Based on their findings, they suggest that
several evaluators and domain experts should participate in evaluating critical
software.

Identifying usability problems on a physiological, or psychological level, has been shown to be just as difficult. In a
shift towards a focus on \textit{user experience}, several attempts have been made at identifying usability problems
from the perspective of how test participants are affected by the experience of interacting with a system. In other
words, the affective state of a test participant might reveal potential problems within a software system. Jussi
P.P. Jokinen~\cite{workplace_up_study} investigated how frustration plays a key-role in how individuals interact with a
system. Likewise, Lazar et al.~\cite{frustration_with_computers} found that text-processing and email-related tasks
induces the highest amount of frustration.

\subsection{Sensors}
Sensors has been used in various scenarios in the HCI field, spanning from input devices for games to detecting the well being of a person.
\todo{se latex comment for inspiration}
%Ved ikke lige helt hvordan det her skal skrives godt
%Noget i stilen med at sensors er et meget kendt phenomen inden for HCI, brug til bla bla bla et par cites. Mere interessant er det blevet brugt til at establishe effective state p√• personer - Dog med forskellige resultater og stimuli. (many cites)

\subsection{Sensors and usability testing}
While still a relatively unexplored area within HCI research,
physiological sensors are increasingly being applied in usability experiments.
Elling et at.~\cite{concurrent_think_aloud_eye_tracking} investigated the
relationship between what users verbalized during think-aloud sessions and where
their gaze was at (i.e. used eye-tracking equipment), to both scrutinize the
think-aloud method and to complement it. Similarly, P\"{a}tsch et
al.~\cite{using_sensor_graphs_think_aloud} complimented think-aloud recall
sessions using GSR sensor data.

%Brug det shitty liapis paper her

As mentioned in the introduction, Bruun et al. combined both GSR sensor and
eye-tracking data to compliment think-aloud recall sessions. They found a
significant correlation between moments of peaks in GSR readings and instances
of usability problems. They results were however encumbered by the
\textit{evaluator effect} and conclude that a structured and operationalized
method for capturing affective state and associated it with usabilty problem
severity is still needed within HCI.

It is clear from the literature and recent research trends within the HCI community, that a revision of the traditional usability evaluation methods is being explored. \todo{er det clear though?}
Current methods rely on ''experts' and their subjective analytic abilities whilst being focused on performance-based metrics.
As exemplified, more research is taking place within the area of using and applying sensors to estimate the affective state of users, however, this research is limited and usually only one sensor is used.\todo{Hvor har vi det fra?}